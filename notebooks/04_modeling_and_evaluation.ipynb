{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a64e02",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation – Customer Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaaa276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "# Get the path of the Python interpreter being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# Get current working directory\n",
    "os.chdir(r\"C:\\Users\\MVS Sastri\\Desktop\\customer churn analytics\")\n",
    "# Change working directory to the project folder   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a300e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dee7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/processed/cleaned_telco_churn.csv\")\n",
    "df.head()\n",
    "#loaded the cleaned dataframe from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "#encoded categorical variables using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()\n",
    "#get a concise summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(\"Churn\", axis=1)\n",
    "y = df_encoded[\"Churn\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    \n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "#split data into training and testing sets, features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "#trained a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (Logistic Regression):\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix (Logistic Regression):\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report (Logistic Regression):\\n\", classification_report(y_test, y_pred))\n",
    "#evaluated the model's performance using accuracy, confusion matrix, and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234549a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_recall = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight={0: 1, 1: 2}   # churn is more important\n",
    ")\n",
    "\n",
    "model_recall.fit(X_train, y_train)\n",
    "#trained a logistic regression model with adjusted class weights to prioritize recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_recall = model_recall.predict(X_test)\n",
    "print(\"Accuracy (Recall Model):\", accuracy_score(y_test, y_pred_recall))\n",
    "print(\"\\nConfusion Matrix (Recall Model):\\n\", confusion_matrix(y_test, y_pred_recall))\n",
    "print(\"\\nClassification Report (Recall Model):\\n\", classification_report(y_test, y_pred_recall))\n",
    "#evaluated the recall-prioritized model's performance using accuracy, confusion matrix, and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa97362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#standardized the feature data using StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad78214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight={0: 1, 1: 2},\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "#trained a logistic regression model on standardized data with adjusted class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd075e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy (Scaled Model):\", accuracy_score(y_test, y_pred_scaled))\n",
    "print(\"\\nConfusion Matrix (Scaled Model):\\n\", confusion_matrix(y_test, y_pred_scaled))\n",
    "print(\"\\nClassification Report (Scaled Model):\\n\", classification_report(y_test, y_pred_scaled))\n",
    "#evaluated the standardized model's performance using accuracy, confusion matrix, and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f97eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e032eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    class_weight={0: 1, 1: 2}   # still care more about churn\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#trained a random forest model with adjusted class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57762755",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (Random Forest):\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix (Random Forest):\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report (Random Forest):\\n\", classification_report(y_test, y_pred_rf))\n",
    "#evaluated the random forest model's performance using accuracy, confusion matrix, and classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82add16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "#threshold tuning for random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "#obtained predicted probabilities for the positive class using the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred_rf_tuned = (y_prob_rf >= 0.35).astype(int)\n",
    "#applied threshold tuning to the random forest model's predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (RF Tuned):\", accuracy_score(y_test, y_pred_rf_tuned))\n",
    "print(\"\\nConfusion Matrix (RF Tuned):\\n\", confusion_matrix(y_test, y_pred_rf_tuned))\n",
    "print(\"\\nClassification Report (RF Tuned):\\n\", classification_report(y_test, y_pred_rf_tuned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27447452",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(\"Churn\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get reports as dictionaries\n",
    "report_logreg = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_recall = classification_report(y_test, y_pred_recall, output_dict=True)\n",
    "report_scaled = classification_report(y_test, y_pred_scaled, output_dict=True)\n",
    "report_rf_tuned = classification_report(y_test, y_pred_rf_tuned, output_dict=True)\n",
    "#compiled classification reports from different models into a single dataframe for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(report_scaled).T\n",
    "pd.DataFrame(report_logreg).T\n",
    "pd.DataFrame(report_recall).T\n",
    "pd.DataFrame(report_rf_tuned).T\n",
    "#displayed the classification report for the standardized logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"Logistic Regression (Baseline)\",\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision_churn\": report_logreg[\"1\"][\"precision\"],\n",
    "        \"recall_churn\": report_logreg[\"1\"][\"recall\"],\n",
    "        \"f1_churn\": report_logreg[\"1\"][\"f1-score\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Logistic Regression (Class-Weighted)\",\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_recall),\n",
    "        \"precision_churn\": report_recall[\"1\"][\"precision\"],\n",
    "        \"recall_churn\": report_recall[\"1\"][\"recall\"],\n",
    "        \"f1_churn\": report_recall[\"1\"][\"f1-score\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Logistic Regression (Scaled + Weighted)\",\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_scaled),\n",
    "        \"precision_churn\": report_scaled[\"1\"][\"precision\"],\n",
    "        \"recall_churn\": report_scaled[\"1\"][\"recall\"],\n",
    "        \"f1_churn\": report_scaled[\"1\"][\"f1-score\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Random Forest (Tuned)\",\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_rf_tuned),\n",
    "        \"precision_churn\": report_rf_tuned[\"1\"][\"precision\"],\n",
    "        \"recall_churn\": report_rf_tuned[\"1\"][\"recall\"],\n",
    "        \"f1_churn\": report_rf_tuned[\"1\"][\"f1-score\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "model_comparison.to_csv(\"models/model_comparison.csv\", index=False)\n",
    "#created a summary dataframe comparing key metrics across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_scaled)\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=[\"No Churn\", \"Churn\"]\n",
    ").plot(cmap=\"Blues\")\n",
    "\n",
    "plt.title(\"Confusion Matrix – Final Logistic Regression Model\")\n",
    "plt.show()\n",
    "#visualized the confusion matrix for the standardized logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.bar(\n",
    "    model_comparison[\"model\"],\n",
    "    model_comparison[\"recall_churn\"]\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Recall (Churn)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.title(\"Model Performance Comparison – Churn Recall\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Get feature importance from logistic regression\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": abs(model_scaled.coef_[0])\n",
    "})\n",
    "\n",
    "# Sort and select top 10\n",
    "feature_importance = feature_importance.sort_values(\n",
    "    by=\"importance\", ascending=False\n",
    ").head(10)\n",
    "\n",
    "# Create visuals folder if not exists\n",
    "os.makedirs(\"visuals\", exist_ok=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(\n",
    "    feature_importance[\"feature\"],\n",
    "    feature_importance[\"importance\"]\n",
    ")\n",
    "plt.xlabel(\"Importance (Absolute Coefficient Value)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Feature Importance – Logistic Regression\")\n",
    "plt.gca().invert_yaxis()  # most important on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(\"visuals/feature_importance_logistic.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(\"Churn\", axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
